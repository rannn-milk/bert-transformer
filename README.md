# bert-transformer
<meta charset="UTF-8">
项目框架
===
project/
│
├── dataset/
│   ├── 2017 (1.csv
│   └── 数学题库final.csv
│
├── bert.py
│   - 负责BERT模型的加载、文本数据处理以及词向量生成等功能。
│
├── transformer1.py
│   - 定义了Transformer架构的编码器、解码器以及相关组件。
│
├── dataload.py
│   - 实现数据加载和预处理的逻辑，将原始数据转换为模型可接受的格式。
│
├── mask.py
│   - 生成各种掩码，如填充掩码、后续掩码，用于处理序列数据中的填充值和防止模型看到未来信息。
│
├── train.py
│   - 包含模型训练的主要逻辑，定义训练函数，计算损失并更新模型参数。
│
├── main.py
│   - 项目的主程序文件，负责参数解析、数据加载、模型训练、评估和测试等整个流程的控制。
│
├── README.md
│   - 本项目说明文档。

    
架构
==
BERT 模块
--
文本特征提取：利用预训练的 BERT 模型，对题目文本（如Q_Title）进行深度语义理解与特征提取。通过 BERT 分词器将文本转化为模型可处理的输入格式，然后获取文本的词向量表示，这些向量蕴含了丰富的语义信息，为后续的模型学习提供基础。

Transformer 模块
--
编码器（Encoder）：负责对学生的答题序列数据进行编码。包括问题 ID、技能 ID、答题正确性、答题时间等多维度信息，通过嵌入层将这些信息转化为向量表示，并结合位置编码，使模型能够捕捉序列中的位置信息。经过多层多头注意力机制和前馈神经网络的处理，提取出有效的特征表示。
解码器（Decoder）：根据编码器的输出以及目标序列（如预测学生下一次答题的正确性）的部分信息，生成最终的预测结果。在生成过程中，利用掩码机制防止模型提前看到未来的信息，保证预测的合理性。
